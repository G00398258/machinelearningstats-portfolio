{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ece8f76-cdd2-45a8-832c-c26194957ccb",
   "metadata": {},
   "source": [
    "# Keras: Time Series Anomaly\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78abd5-bffc-45af-bcee-dae19a82373e",
   "metadata": {},
   "source": [
    "This Jupyter notebook will contain my submission for the project aspect of the assessment for the Machine Learning & Statistics module (Winter 2022).  \n",
    "<br>  \n",
    "The focus of this project will be time-series anomaly detection using Keras. In this notebook, I have recreated and broken down the example code from the [Keras website](https://keras.io/examples/timeseries/timeseries_anomaly_detection/) (with minor changes suggested by the lecturer) into four sections:  \n",
    "1. Data  \n",
    "2. Pre-Processing  \n",
    "3. Neural Network  \n",
    "4. Evaluation    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69477d-d3ab-4924-b292-a918e236212b",
   "metadata": {},
   "source": [
    "Under each section, I will give an overview of what is happening in the code and what the user can expect to see, and aim to clearly explain each Keras function used.  \n",
    "<br>  \n",
    "Finally, I suggest some improvements that could be made to the analysis. The notebook concludes with a list of references referred to and consulted in the completing of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c29031-7fb8-4439-8041-beb806988496",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8832e3-ff7d-469d-bf74-9e9c26262c7f",
   "metadata": {},
   "source": [
    "# From Ian's Notebook\n",
    "\n",
    "***\n",
    "\n",
    "https://keras.io/examples/timeseries/timeseries_anomaly_detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb682d0-de64-4afc-b656-854a9b004eb6",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ba0902-0576-4608-a726-a1c3b4b8e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical arrays\n",
    "import numpy as np\n",
    "\n",
    "# Spreadsheet-like Data Frames\n",
    "import pandas as pd\n",
    "\n",
    "# Neural networks\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f045b6-d1a6-40f5-ac64-120eef018491",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c8e51-b0f9-4ecf-9281-09a2abc97623",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "***\n",
    "\n",
    "https://www.kaggle.com/datasets/boltzmannbrain/nab\n",
    "\n",
    "https://github.com/numenta/NAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55829dcf-a72e-4642-88ae-a35cfea3a495",
   "metadata": {},
   "source": [
    "The data used in this project is artificial data taken from the Numenta Anomaly Benchmark (NAB) dataset. Introduced in 2015, NAB is an open source framework comprised of labeled data files which uses a common scoring system \"_to compare and evaluate different anomaly detection algorithms for detecting anomalies in streaming data_\" [fintelics]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff1237-45ff-4502-8614-c332c612eee4",
   "metadata": {},
   "source": [
    "In this section, we will gather the data for the project, import the data into dataframes, check that the data looks OK and do an inital plot of the data so we can take a look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849b0151-072d-4a18-abf8-7d29b0ff348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root of URLs\n",
    "root_url = 'https://raw.githubusercontent.com/numenta/NAB/master/data/'\n",
    "\n",
    "# Data file without anomaly\n",
    "no_anomaly_url = root_url + 'artificialNoAnomaly/art_daily_small_noise.csv'\n",
    "\n",
    "# Data file with anomaly\n",
    "with_anomaly_url = root_url + 'artificialWithAnomaly/art_daily_jumpsup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef2ef2c-9d9e-49d0-ae7b-0ba3b156ad44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://raw.githubusercontent.com/numenta/NAB/master/data/artificialNoAnomaly/art_daily_small_noise.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the no-anomaly URL looks OK\n",
    "no_anomaly_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a40518-51c9-4266-aa76-f2b3d76becb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://raw.githubusercontent.com/numenta/NAB/master/data/artificialWithAnomaly/art_daily_jumpsup.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the with anomaly URL looks OK\n",
    "with_anomaly_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2829a112-226c-4986-8199-03b56af2e587",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1353\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[0;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1417\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    920\u001b[0m         \u001b[1;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m         self.sock = self._create_connection(\n\u001b[0m\u001b[0;32m    922\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3614609d519c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading the no-anomaly data into a data frame called small noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_small_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_anomaly_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;31m# open URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    287\u001b[0m                 \u001b[1;34m\"storage_options passed with file object or non-fsspec file path\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             )\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Content-Encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urllib.Request'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[0;32m    543\u001b[0m                                   '_open', req)\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1397\u001b[1;33m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[0;32m   1398\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "# Reading the no-anomaly data into a data frame called small noise\n",
    "df_small_noise = pd.read_csv(no_anomaly_url, parse_dates=True, index_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b41bea-fc6b-40d4-83b1-92b035c8d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the top 5 values in the data frame\n",
    "df_small_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4fbee-dce3-4a30-ab97-0b9daf519ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using describe to check how many lines are in the data & see some basic stats\n",
    "df_small_noise.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d5253-e607-4af5-b28d-283302fc64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the first value in the data frame\n",
    "#df_small_noise.iloc[0]\n",
    "#df_small_noise.iloc[0].values[0]\n",
    "df_small_noise.iloc[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cc33f-345c-4221-814d-46c8c50f844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use shape method to see how many values are in the data frame\n",
    "df_small_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd187c98-a1aa-4521-bb69-07d302405795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Use pandas to plot the data frame\n",
    "df_small_noise.plot(ax=ax);\n",
    "\n",
    "# showing the legend\n",
    "plt.legend(['No Anomalies'], loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1214e-a97b-497a-bf9d-f33c28823130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the with anomaly data into a data frame called daily jumpsup\n",
    "df_daily_jumpsup = pd.read_csv(with_anomaly_url, parse_dates=True, index_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e58d1b-ed89-4e5c-85b0-537891be1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the top 5 values\n",
    "df_daily_jumpsup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162c998-fdf7-45f6-a132-a06ab0700c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the describe method to see the no. of values & other basic stats of the date frame\n",
    "df_daily_jumpsup.describe()\n",
    "\n",
    "#df_daily_jumpsup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecfada0-5a5c-4c56-b6ca-50d2367579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first value in the second data frame\n",
    "df_daily_jumpsup.iloc[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28941db6-dcce-459b-8b1c-913db9396c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating another subplot & setting figsize\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Use pandas to plot the second data frame\n",
    "df_daily_jumpsup.plot(ax=ax);\n",
    "\n",
    "plt.legend(['With Anomalies'], loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca7d6b-2f53-4fea-8433-1103ec811be5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02b156-3b72-4d3c-a573-fa93f5b64e23",
   "metadata": {},
   "source": [
    "## 2. Pre-Processing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80370b-7d65-4d33-9e1f-0d58f07d53d6",
   "metadata": {},
   "source": [
    "Here, we prepare the data we are going to use to train the model by doing calculations on the dataset with no-anomaly using the mean and standard deviation methods, and storing the data in windows inside a value called x_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609d490-ec4f-4d18-885b-adb23ae50436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of the no-anomaly dataset & store it in var train_mean\n",
    "train_mean = df_small_noise.mean()\n",
    "\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649276a-e36a-44fa-b9ff-a42cc4473f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the standard deviation of the no-anomaly dataset & store it in var train_std\n",
    "train_std = df_small_noise.std()\n",
    "\n",
    "train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b8a89-1984-4ebe-a648-bff824e9cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the difference between each no-anomaly value & its mean by the standard deviation\n",
    "# store the result in a new data frame called df_train_vals\n",
    "df_train_vals = (df_small_noise - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22cf41-fbdc-4469-836e-a159b2d9dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the top 5 values in the new data frame\n",
    "df_train_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd8797-5d3c-4037-b317-0f1f432d964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new cols to the no-anomaly dataframe containing the value - mean (less_mean) and\n",
    "# the new less_mean value divided by the standard deviation (div_std)\n",
    "df_small_noise['less_mean'] = df_small_noise['value'] - df_small_noise['value'].mean()\n",
    "df_small_noise['div_std'] = df_small_noise['less_mean'] / df_small_noise['value'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e47961-8b63-4429-854f-3a00b5874211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see what the no-anomaly data frame looks like now\n",
    "df_small_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80f33a-cbbe-46ab-85f7-76c908bbe75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the value, less_mean and div_std columns in the data frame\n",
    "df_small_noise['value'].mean(), df_small_noise['less_mean'].mean(), df_small_noise['div_std'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07ccbb-5e40-47bc-9009-91704a98b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the value, less_mean and div_std columns in the data frame\n",
    "df_small_noise['value'].std(), df_small_noise['less_mean'].std(), df_small_noise['div_std'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10130802-24b7-439d-85fa-61da237d0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7068e-28a6-4df4-b794-3395a3a9479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "# Plotting the train_vals data frame with pandas\n",
    "df_train_vals.plot(ax=ax);\n",
    "\n",
    "plt.legend(['Training Values'], loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e802df5-e98f-4b33-93a2-1081bd464449",
   "metadata": {},
   "source": [
    "Compare this plot to the previous one where we plotted df_small_noise (the no-anomaly dataset). From a quick glance, the two look the same but upon investigating the values you can see that we have essentially scaled the original values down to be centred around zero, with a standard deviation of approximately 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71149c8d-3e77-45cb-8733-fa65fef90c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using shape to see how many values are in the train_vals data frame\n",
    "df_train_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81582a-2d70-49bc-9510-22e045cceec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to break the train_vals into sequences or \"windows\" so we can look at each value\n",
    "# in relation to its peers - here we determine the window size will be 288\n",
    "window_size = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618db5eb-0006-49db-8e53-fef7f28b2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to split the data into windows of size 288\n",
    "def windows(vals, n=window_size):\n",
    "  # creating a list L to store the windows\n",
    "  L = []\n",
    "  # looping through the data up to range 3745 (where the last window begins - 4032-288+1)\n",
    "  for i in range(len(vals) - n + 1):\n",
    "    # append each window to L\n",
    "    L.append(vals[i:i+n])\n",
    "  # stack method will create a 2D numpy array from L\n",
    "  return np.stack(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f3571-9ef4-440f-81fb-5491bd1a50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the train_vals to the windows function & store result in var x_train\n",
    "x_train = windows(df_train_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df214bec-6bc4-47cc-b187-197df2f33c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking value in first index of first window and last index of last window\n",
    "x_train[0][0], x_train[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29feea5e-50b2-4a81-8ffc-0538e17e9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking first and last value in df_train_vals - notice these are the same as above\n",
    "df_train_vals.iloc[0], df_train_vals.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a51069-5f67-4c81-8cb8-a8e72ee896e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using shape to check the # of values in x_train\n",
    "# note that 3745 + 288 -1 = 4032 (the # values in train_vals)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5d1cb-8045-46c0-b2c9-6d84a81ae9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set value for window number - this will be used as an index below\n",
    "window_no = 200\n",
    "\n",
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# set y = the values in the train_vals data frame\n",
    "y = df_train_vals['value'].values\n",
    "\n",
    "# plot y with label signal\n",
    "ax.plot(np.arange(y.shape[0]), y, label='Signal')\n",
    "\n",
    "# set var w = window at index 200 in x_train - flatten will remove the inner arrays\n",
    "w = x_train[window_no].flatten()\n",
    "\n",
    "# plot w on top of y with label window\n",
    "ax.plot(np.arange(w.shape[0]) + window_no, w, label='Window')\n",
    "\n",
    "# show the legend, position in upper right corner\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html\n",
    "ax.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838160d-18fc-4468-9955-aa8618176a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that you can change which window gets plotted - see how the orange area moves along the x-axis\n",
    "window_no = 250\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(np.arange(y.shape[0]), y, label='Signal')\n",
    "\n",
    "# set var w = window at index 200 in x_train - flatten will remove the inner arrays\n",
    "w = x_train[window_no].flatten()\n",
    "\n",
    "# plot w on top of y with label window\n",
    "ax.plot(np.arange(w.shape[0]) + window_no, w, label='Window 250')\n",
    "\n",
    "# show the legend\n",
    "ax.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d707f-e507-475d-9046-9640d1979114",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc6ff22-e1ff-4840-9e6d-174e3a3970d2",
   "metadata": {},
   "source": [
    "## 3. Neural Network\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23092c9-440a-4a2a-875c-dcfd82d323b8",
   "metadata": {},
   "source": [
    "Now that we have our training data, we need to create the neural network for our model. This section is probably the most complicated of all sections in this notebook, so I will spend some time here explaining what is happening at each stage before getting into the code.  \n",
    "<br>  \n",
    "We begin building our neural network by defining the layers, which are described as \"_the primary building block of Keras models_\" by TutotialsPoint, with each layer receiving input, performing some calculation and outputting the result to the next layer []. We can then optimize and compile the model.  \n",
    "<br>  \n",
    "The layers can be defined as follows:\n",
    "##### Input  \n",
    "- Here's what the [Keras documentation](https://keras.io/api/layers/core_layers/input/) tells us about the input layer:  \n",
    "\"_Input() is used to instantiate a Keras tensor. A Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model._\"[keras docs]  \n",
    "- The input layer is literally where we input data into our model. With the input layer, we need to specify the shape of the input data we are going to use to train the model in the form of tuples[wandb], so in our example the input shape has sequence_length of 288 and num_features 1. [keras docs]  \n",
    "\n",
    "##### Conv1D  \n",
    "- The next layer applied to our model is called Conv1D. According to [this GitHub post](https://kitchell.github.io/DeepLearningTutorial/4cnnsinkeras.html), the convolutional layer does the 'heavy lifting' within our neural network. In this layer, each filter (32 in our case) is slid (or convolved) along the input, and as this happens an activation or feature map is produced giving the response of the filter at each position. The author goes on to explain that \"_the network will learn filters that activate when they see a specific feature, such as edges if the input is an image. Each convolutional layer has multiple filters and each filter produces a separate activation or feature map. These activation maps are stacked together and passed to the next layer._\"[kitchell] In our model we are using a one dimensional Convolutional Layer, but 2D versions also exist.  \n",
    "- Note that when we create the Conv1D layer, we use the parameter activation=\"relu\", which stands for rectified linear unit. In their blogpost, kitchell tells us that relu sets all negative pixels or elements of the activation to zero, which introduces non-linearity into the data to make it more like real-life data, and helps to balance out the linear convolution process.[kitchell]  \n",
    "<br>  \n",
    "  \n",
    "##### Dropout  \n",
    "- The dropout layer is next. Jason Brownlee at machinelearningmastery.com tells us that dropout is a regularization technique for neural networks in which randomly selected neurons are ignored (or \"dropped off\") during training. [mlm dropout] It is not used when evaluating the model. [mlm dropout] The contribution of the dropped neurons to the activation of downstream neurons is temporally disabled on the forward pass, and any affect on the weights is not applied on the backward pass.[projectpro]  \n",
    "- Brownlee provides the following explanation on the effect of the dropout technique:  \n",
    "\"_As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features, providing some specialization. Neighboring neurons come to rely on this specialization, which, if taken too far, can result in a fragile model too specialized for the training data. This reliance on context for a neuron during training is referred to as complex co-adaptations.  \n",
    "You can imagine that if neurons are randomly dropped out of the network during training, other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.  \n",
    "The effect is that the network becomes less sensitive to the specific weights of neurons. This, in turn, results in a network capable of better generalization and less likely to overfit the training data._\"[mlm dropout]  \n",
    "- In our model, we are using a dropout rate of 0.2. After this layer has been applied, another Conv1D layer is created, with 16 filters this time.  \n",
    "<br>  \n",
    "\n",
    "##### Conv1DTranspose  \n",
    "- After the second Conv1D layer has been applied to the model, we then add a Conv1DTranspose layer. Here's what the [Keras documentation](https://keras.io/api/layers/core_layers/input/) tells us about this layer:  \n",
    "\"_Transposed convolution layer (sometimes called Deconvolution).The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of its input while maintaining a connectivity pattern that is compatible with said convolution._\"  \n",
    "- This was the layer I found most difficult to understand, but based on the definition cited above and my understanding of how transpose works in Excel, I take it that in this step we are flipping the data that has been passed in the previous Conv1D step on it's head before it get's passed to the next layer.  \n",
    "- Following this step, a second dropout layer is applied to the model, followed by two more Conv1DTranspose layers.  \n",
    "<br>  \n",
    "\n",
    "Once the layers have been defined, we can then create our model by passing the layers to Keras. In this example, we use a sequential model. The idea is that the layers we pass to the model are arranged in order (sequence) with the data flowing from one layer to the next until it reaches the output layer. [https://www.tutorialspoint.com/keras/keras_models.htm]  \n",
    "<br>  \n",
    "\n",
    "The next step is optimization. The optimizer or optimization algorithm is the logic which the neural network uses to decide how much adjustment is needed on the weights on the nodes to solve the problem at hand, based on the difference between the training data and the results observed. [https://www.kaggle.com/code/residentmario/keras-optimizers]. We will use an optimizer called Adam, whose name is devired from adaptive moment estimation. [keras same]. While Adam is a form of stochastic gradient descent, it differs from the classical interpretation as the learning rate of the optimizer can adapt and do at different rates for each eight as learning unfolds. [mlmadam]  \n",
    "<br>  \n",
    "\n",
    "Finally, with the layers, model type and optimizer defined, the last step is to choose a loss function for our model. The purpose of the loss function is to evaluate the weights and aim to minimize the error (or loss), with less error equating to a more accurate neural network. [dataflair] The loss function we will use is called mse (mean squared error) and is calculated as the average of the squared differences between the predicted and actual values. [mlm htcl] This particular loss function punishes larger errors  more than small ones. [neptune]  \n",
    "<br>  \n",
    "\n",
    "We can then compile our model and take a look at it's summary information, before training the model. In training, we use x_train as both the input and the output (as we are using is a reconstruction model)[https://keras.io/examples/timeseries/timeseries_anomaly_detection/], and set epochs, which is number of times we want the model to be evaluated during training to 50 and batch size (the number of training instances) to 128. We also pass the argument validation_split=0.1, which according to the [offical Keras documentation](https://keras.io/api/models/model_training_apis/) is a \"_fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch._\"[tpkmc].  \n",
    "<br>  \n",
    "\n",
    "The final argument we use in the training step is callbacks, with the class EarlyStopping, and we pass the following arguments: monitor=\"val_loss\", patience=5, mode=\"min\". With this argument, we are telling the model that it can stop the training when a monitored metric has stopped improving, and in this case we are telling it monitor the quantity of \"val_loss\", with patience (the number of epochs after which to stop the training if there is no improvement) set to 5 mode set to min, meaning training will stop when val_loss has stopped decreasing. [keras callbacks]  \n",
    "<br>\n",
    "\n",
    "Now, let's get into the code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132440ee-d7b0-4bae-a1a9-ce1654bba5cc",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc87cef-4f71-4977-bf89-e3586abd25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the shape of our training data again - we'll use this in the input layer\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca0681-2def-490b-a7a0-1033d581fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the layers of our neural network\n",
    "layers = [\n",
    "  # input will be 288 (sequence length) & 1 (num_features) -> index 1 and 2 of the shape value above\n",
    "  keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "  # 1st Conv1D layer with 32 filters and stride (paces to slide along data) of 2\n",
    "  keras.layers.Conv1D(\n",
    "    filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  # 1st dropout layer - ignoring/dropping off 2% of the neurons in the data\n",
    "  keras.layers.Dropout(rate=0.2),\n",
    "  # 2nd Conv1D layer - 16 filters this time but same stride\n",
    "  keras.layers.Conv1D(\n",
    "    filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  # 1st Conv1DTranspose layer where data is essentially flipped\n",
    "  keras.layers.Conv1DTranspose(\n",
    "    filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  # 2nd dropout layer, same rate\n",
    "  keras.layers.Dropout(rate=0.2),\n",
    "  # 2nd Conv1DTranspose layer\n",
    "  keras.layers.Conv1DTranspose(\n",
    "    filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  # 3rd Conv1DTranspose layer\n",
    "  keras.layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02986e7f-7d3f-4ee2-9d6d-2a584e455c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the layers to keras to create a sequential model\n",
    "model = keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2abc2-908e-4159-816e-b2d4be37ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Adam optimizer with learning rate 0.001 (this is the default learning rate)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e647861-f74b-427d-98f4-af74d1e7642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the module with our Adam optimizer & mean squared error loss function\n",
    "model.compile(optimizer=optimizer, loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08495e8-b6fa-497b-9225-7925eafb456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of our model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c8d22-cf9e-4fde-8438-35dc6744a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e7e54-cdf6-45f9-b62b-a4bcbf80fa59",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585ddd3-739a-4d04-8b09-77f0d8c3f277",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5065420-d192-4422-8690-71711f2b5d3b",
   "metadata": {},
   "source": [
    "In this final section of code, we evaluate the performance of our model and use it to detect anomalies in our data by determining how well it can reconstruct the input data. [keras ex]  \n",
    "<br>  \n",
    "\n",
    "We first plot the values of loss (being the loss observed on the training data) against the validation loss to see how the training went. We then use our newly trained model to make predictions based on the training data and calculate the mean absolute error (MAE) loss, being the absolute difference between the predictions and actual values. [keras ex] We will then find the maximum MAE loss value, which is the worst performance point of our model in trying to reconstruct the data and make this the threshold for anomaly detection, using the logic that \"_if the reconstruction loss for a sample is greater than this threshold value then we can infer that the model is seeing a pattern that it isn't familiar with_\" (i.e. an anomaly). [keras ex]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f43bf-0fd3-47aa-b45b-f9fca245b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(1, 6))\n",
    "\n",
    "# plotting (training) loss\n",
    "ax.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "# plotting validation loss\n",
    "ax.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "\n",
    "# show the legend\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65009986-a249-4b88-9d6a-c4cb069670c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to make predictions\n",
    "x_train_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f2416-255c-4cc8-9610-8dbb35435f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE loss as absolute value of the predictions minus the training values\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646b9d6-c403-4ca3-bb2b-de4adc625176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# plot MAE loss values\n",
    "ax.hist(train_mae_loss, bins=50)\n",
    "\n",
    "# setting label on x axis\n",
    "ax.set_xlabel(\"Train MAE Loss\")\n",
    "# setting label on y axis\n",
    "ax.set_ylabel(\"No. of Samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce87b24-d218-4e9f-95da-113a004f67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use max MAE value as reconstruction loss threshold\n",
    "threshold = np.max(train_mae_loss)\n",
    "\n",
    "# have a look at the threshold\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f16149-8d3a-408d-b300-026f494d713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to take a look at the values before plotting them to make sense of the below plot\n",
    "\n",
    "#len(x_train[0])\n",
    "#x_train[0]\n",
    "#len(x_train_pred[0])\n",
    "#x_train_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057837a-55f4-4367-88ca-5654c9867dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# plotting first window of the training data (288 values)\n",
    "ax.plot(x_train[0], label=\"Training Data\")\n",
    "\n",
    "# plotting first window of the predictions\n",
    "ax.plot(x_train_pred[0], label=\"Predictions\");\n",
    "\n",
    "# show the legend\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459f2f9-7628-42aa-829e-e3fd7363f7ce",
   "metadata": {},
   "source": [
    "We can see that the model was able to make predictions that were quite close to the training data, and did actually overlap with the training data in places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60478b-db7d-4316-b8a4-e64fdf35260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the values in the with-anomalies dataset down, as we did with the no-anomaly data earlier\n",
    "# store values in a new data frame called test_value\n",
    "df_test_value = (df_daily_jumpsup - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792db028-2713-45f8-b154-da0c923941a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# plot the test values in the with-anomalies dataset\n",
    "df_test_value.plot(ax=ax)\n",
    "\n",
    "plt.legend(['Test Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff1b66e-ddf7-41ed-bb4b-df3b1669d0cd",
   "metadata": {},
   "source": [
    "From looking at the above chart, we can see that there is a point at which the data seems to \"jump up\" (hence the name we gave to the data frame) somewhere between timestamps 11 and 12, but with a larger data set or one that has less variation between values, this would not be as obvious.\n",
    "<br>  \n",
    "\n",
    "We know that the model can't just look at a chart like we can and see if anything stands out, so what we want to see now is if the model can identify the anomalies in the data based on the training we have implemented.  \n",
    "<br>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25251571-5370-4bf3-869f-9fc71d8a908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences (windows) from the test values by passing them to the windows function\n",
    "x_test = windows(df_test_value.values)\n",
    "\n",
    "# check the shape - it should be the same as the shape of the no-anomalies data\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd03777-2c89-40fc-b1b1-a49ce9fdc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model to make predictions based on the new test windows\n",
    "x_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba52b6-af40-4c5c-a1f8-dc88488fc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE test loss as absolute value of the test predictions minus the test values\n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "\n",
    "#test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9ad0a-113f-46b4-9541-d07f5fc9fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping into a 1D array - https://www.codingem.com/numpy-reshape-minus-one/\n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d6cac-deeb-4def-adeb-1fbf1a0d6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# plot the test MAE loss values\n",
    "ax.hist(test_mae_loss, bins=50)\n",
    "\n",
    "# set label on x axis\n",
    "ax.set_xlabel(\"Test MAE Loss\")\n",
    "\n",
    "# set label on y axis\n",
    "ax.set_ylabel(\"No. of Samples\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fa3f2-e125-462a-bee0-bdada00d6b8a",
   "metadata": {},
   "source": [
    "This plot looks quite different to the histrogram we plotted earlier using the train MAE loss values so we know that something is off in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab9fdb-3db6-4d50-8559-17d21db75f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all the samples which are anomalies based on the threshold set previously\n",
    "anomalies = test_mae_loss > threshold\n",
    "\n",
    "# Number of anomalies\n",
    "np.sum(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfcb81-107e-44c5-989f-9465b9a48832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested the values used in the next cell out to help me understand what was happening\n",
    "\n",
    "# window_size -1\n",
    "# = 287\n",
    "\n",
    "# len(df_test_value)\n",
    "# = 4032\n",
    "\n",
    "# len(df_test_value) - window_size + 1\n",
    "# = 3745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe0d2b-4039-48e3-a92c-6319f0da28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list to store the indices of the anomalies\n",
    "anomaly_indices = []\n",
    "\n",
    "# logic of function below: index i is an anomaly if samples (i - timesteps + 1) to (i) are anomalies \n",
    "# i.e. if all elements in the window are above the threshold\n",
    "\n",
    "# loop through test values from range 287 to 3745\n",
    "for i in range(window_size - 1, len(df_test_value) - window_size + 1):\n",
    "    # if all values are present (true) in the corresponding anomalies windows, append them to anomaly_indices list\n",
    "    # https://www.sharpsightlabs.com/blog/numpy-all/\n",
    "    if np.all(anomalies[i - window_size + 1 : i]):\n",
    "        anomaly_indices.append(i)\n",
    "        # I had a look at those indices - commenting out as it's quite long!\n",
    "        #print (anomalies[i - window_size + 1 : i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33363e0-a623-4c45-ae91-78caf5676b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use iloc on the with-anomalies data frame to isolate the anomaly indices in a new subset data frame\n",
    "df_subset = df_daily_jumpsup.iloc[anomaly_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4572524-60a9-40b4-8889-4b86871f69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subplot and set the figsize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# plot the values in the with-anomalies data frame\n",
    "df_daily_jumpsup.plot(ax=ax)\n",
    "\n",
    "# overlay the anomalies on top of the plot in red\n",
    "df_subset.plot(ax=ax, color=\"r\")\n",
    "\n",
    "# show the legend - https://www.statology.org/pandas-plot-legend/\n",
    "plt.legend(['Dataset', 'Anomalies']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69e477-d226-4f09-89a8-41d24e81e19b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b19fb-19d1-4e2d-bc8e-431847d02c5a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e7cb0-f136-4c43-b7d7-2b134a8da9d7",
   "metadata": {},
   "source": [
    "In conclusion...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372d0a3-fdba-4c22-a630-199a366c90e1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b7b14-154e-4f75-ba12-adbb4c3e7ab6",
   "metadata": {},
   "source": [
    "## References\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84483a73-c3f7-43e3-a204-00fefcb3f5ea",
   "metadata": {},
   "source": [
    "[] https://data-flair.training/blogs/keras-loss-functions/  \n",
    "[] https://fintelics.medium.com/everything-you-need-to-know-about-numenta-anomaly-benchmark-nab-b43ab7f014df  \n",
    "[] https://www.kaggle.com/code/residentmario/keras-optimizers  \n",
    "[] https://www.kaggle.com/datasets/boltzmannbrain/nab  \n",
    "[] https://keras.io/api/callbacks/early_stopping/  \n",
    "[] https://keras.io/api/models/model_training_apis/  \n",
    "[] https://keras.io/examples/timeseries/timeseries_anomaly_detection/  \n",
    "[] https://keras.io/api/layers/core_layers/input/  \n",
    "[] https://kitchell.github.io/DeepLearningTutorial/4cnnsinkeras.html  \n",
    "[] https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/  \n",
    "[] https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/  \n",
    "[] https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/  \n",
    "[] https://machinelearningmastery.com/lstm-autoencoders/  \n",
    "[] https://medium.com/towards-artificial-intelligence/autoencoder-for-anomaly-detection-using-tensorflow-keras-7fdfa9f3ad99  \n",
    "[] https://neptune.ai/blog/keras-loss-functions  \n",
    "[] https://www.projectpro.io/recipes/what-is-drop-out-rate-keras  \n",
    "[] https://towardsdatascience.com/time-series-of-price-anomaly-detection-with-lstm-11a12ba4f6d9  \n",
    "[] https://www.tutorialspoint.com/keras/keras_layers.htm  \n",
    "[] https://www.tutorialspoint.com/keras/keras_model_compilation.htm  \n",
    "[] https://www.tutorialspoint.com/keras/keras_models.htm  \n",
    "[] https://wandb.ai/ayush-thakur/dl-question-bank/reports/Keras-Layer-Input-Explanation-With-Code-Samples--VmlldzoyMDIzMDU  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703146cb-2a3b-48d2-b329-d1603575977b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b5faa-5039-452f-b671-5d2f9d7a9f58",
   "metadata": {},
   "source": [
    "# End\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
